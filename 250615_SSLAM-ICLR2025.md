
## 🔍 핵심 아이디어: SRL (Source Retention Loss)

**SSLAM**은 기존 Self-Supervised Audio SSL이 모노 음원만으로 훈련된다는 점에 문제의식을 두고, 실제 환경에 더 가까운 **혼합된(polyphonic) 입력**을 자기지도 학습에 도입합니다.

단순히 두 음원을 `max()` 연산으로 혼합하는 데 그치지 않고, **혼합된 입력으로부터 개별 소리의 특징을 잃지 않도록 유도하는 손실함수**, 즉 `Source Retention Loss (SRL)`를 제안합니다.

### 🎯 SRL 개념

* 입력 음원 \$S\_1\$, \$S\_2\$ 를 \$t\$초 길이의 log-mel 스펙트로그램으로 변환합니다.
* 전체 중 절반 구간만 `element-wise max`로 혼합한 \$S\_\text{mixed} = \max(S\_1, S\_2)\$ 를 만듭니다.
* 해당 혼합 구간을 표시하는 바이너리 마스크 \$M\$ 을 생성합니다.

### 👁️ 학생(Student)은 혼합 전체를 본다

* Student 모델은 \$S\_\text{mixed}\$ 전체를 입력으로 받아 마스킹된 패치들을 예측합니다.
* 예측된 혼합 표현은 \$\hat{Y}\_{\text{mixed}}\$입니다.

### 🧠 교사(Teacher)는 두 소스를 **따로** 본다

* \$S\_1\$ → `M = 0` 영역(혼합되지 않은 부분)을 제거한 뒤 teacher에 입력 → 출력 \$Z\_{S\_1}\$
* \$S\_2\$ → 전체 입력 → 출력 \$Z\_{S\_2}\$
* 두 출력을 평균하여 타깃 벡터를 생성합니다.

```math
Z_{\text{target}} = \frac{1}{2} \left( Z_{S_1} + Z_{S_2} \right)
```

### 💡 손실함수 (식 4)

혼합된 입력의 예측 \$\hat{Y}*{\text{mixed}}\$ 와 위의 타깃 \$Z*{\text{target}}\$ 간의 MSE를 계산합니다. 다만 계산은 \$M=1\$ 영역(= 섞인 패치)에서만 수행됩니다.

```math
\mathcal{L}_{\text{SRL}} = \frac{1}{B \cdot n_{\text{MC}} \cdot |M|} \sum_{i,j} \sum_{k \in M} \left\| \hat{Y}_{\text{mixed}}^{(i,j,k)} - Z_{\text{target}}^{(i,k)} \right\|_2^2
```

* \$B\$ : 배치 크기
* \$n\_{\text{MC}}\$ : 마스킹 복제 수
* \$|M|\$ : 마스크된 혼합 패치 수

### ✅ 효과

* 학생이 손실을 줄이려면, **혼합된 표현 안에 두 소스의 고유 특징을 모두 담아야 함**
* 이는 기존 BYOL·MAE 방식에서는 불가능했던 **다중 이벤트 보존 학습**을 가능하게 합니다.

---

## 💬 모노포닉 환경에서는 유효한가?

실제 공장, 경고음, 안전 이벤트 탐지처럼 **“주요 이벤트는 하나지만, 배경음은 존재하는”** 상황에서는 다음과 같은 이유로 **SRL은 여전히 의미 있습니다.**

### 🧩 이유 1: 현실은 진짜 1 소스가 아니다

* SNR이 높아 “한 소리만 있다”고 생각되지만, 실제 녹음에는 기계 소리, 반향, 공진음 등 **잔류 잡음이 섞여 있음**
* SRL은 이러한 **혼합-잡음 조건에 내성 있는 표현**을 배우도록 유도함

### 🧩 이유 2: Partial Mix는 고급 노이즈 마스킹으로 동작

* 일반 MAE는 “빈칸 → 복원”만 수행
* SSLAM은 “빈칸 + 섞임 → 복원 + 분리”를 동시에 학습 → 실제 모노 상황에서도 **잡음 견디는 능력 향상**

### 🧩 이유 3: 모노 성능을 해치지 않음

* Stage 1에서 **unmixed 데이터만**으로 먼저 학습 (10 epoch)
* Stage 2에서만 Partial Mix + SRL을 추가 (5 epoch)
* **모노 정보는 먼저 배우고, polyphonic 강인성은 나중에 얹는 구조**

---

## ⏱️ 학습 비용 비교 (4 × RTX 3090 기준)

| 단계      | Epoch 수 | 시간 / epoch | 총 시간     | GPU-hour      |
| ------- | ------- | ---------- | -------- | ------------- |
| Stage 1 | 10      | 7 h        | ≈ 70 h   | **280 GPU-h** |
| Stage 2 | 5       | 7.5 h      | ≈ 37.5 h | **150 GPU-h** |

* Stage 2만 별도로 실행 가능
* 완전 클린 모노 테스크라면 Stage 2 생략도 가능
* 반대로, **잡음이 얕게라도 섞인 현실형 음원이라면 Stage 2를 2–3 epoch 만 수행해도 효과 있음**

---

## 🧪 요약 – SRL은 언제 효과적인가?

* **겹친 소리**를 동시에 학습하는 Polyphonic 상황에서는 기존 방식 대비 최대 **+9.1 mAP (SPASS)** 의 성능 향상
* **Mono-dominant** 상황에서도 **잡음 내성, 잔향 강건성** 확보 가능
* 특히 잡음이나 배경이 고정되지 않고 다양하게 들어오는 산업·현장 녹음에 효과적


아래에서는 **“Poly Mix SSLAM이 모노포닉-우세(Task 2)에서도 여전히 유효한가?”** 라는 쟁점을 두 축으로 나눠 냉정하게 살펴봅니다.
(모든 수치는 논문 및 OpenReview 본문에서 직접 확인한 내용만 사용합니다.)


##  모노포닉 벤치에서의 실제 효과

| 데이터셋(주로 단일 소스)              | 기존 SOTA(=EAT, MAE-AST 등) | SSLAM        | 차이                                          |
| --------------------------- | ------------------------ | ------------ | ------------------------------------------- |
| **AudioSet-2M** (라벨 ≈ 1/트랙) | 48.3 mAP                 | **50.2 mAP** | **+1.9** ([openreview.net][1])              |
| **ESC-50** (5 s 단일 음)       | 88.0 Top-1               | **88.7**     | **+0.7** (Table 9 인용) ([openreview.net][2]) |
| **KS-2** (키워드)              | 96.1 Acc                 | **96.3**     | **+0.2** (Table 9 인용) ([openreview.net][3]) |


---

🔗 자세한 논문 및 공식 구현:
[🔗 Paper (ICLR 2025)](https://arxiv.org/abs/2403.13028)
[🔗 GitHub Repo](https://github.com/ta012/SSLAM)
[🔗 Model Weights (Google Drive)](https://github.com/ta012/SSLAM#pretrained-models)

